# -*- coding: utf-8 -*-
"""NLP_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cqtx6Jsg3t913rN0eF9eb7rFvPvTy3Ft

#Project 2: Text Classification with Limited Resources


**Objective:** Develop a text classification system that can categorize text into predefined classes, optimized for limited computational resources.

**Tasks:**


1.  Use the IMDB Movie Reviews dataset
2.  Implement data cleaning and preprocessing techniques
3.  Create TF-IDF vectorization for feature extraction
4.  Train and compare multiple classical ML models (Naive Bayes, Logistic Regression, SVM)
5.  Implement a simple neural network using Keras (compatible with free Colab GPUs)
6.  Perform basic hyperparameter tuning and k-fold cross-validation
7.  Create confusion matrices and ROC curves for model evaluation

#**Use the IMDB Movie Reviews dataset**

### **Dataset Description**
*   The IMDb Movie Reviews dataset is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie.
"""

# pip install wordcloud matplotlib
# pip install tabulate

# Importing necessary libraries
import pandas as pd  # For data manipulation and analysis
import numpy as np   # For numerical operations and array handling
import matplotlib.pyplot as plt  # For creating visualizations and plots
import seaborn as sns  # For statistical data visualization
from wordcloud import WordCloud  # For creating word clouds
import nltk  # For natural language processing tasks

# Importing modules for text processing and feature extraction
from sklearn.feature_extraction.text import TfidfVectorizer  # For converting text data into TF-IDF features
from nltk.corpus import stopwords  # For accessing a list of common stop words
import re  # For regular expression operations, often used in text cleaning
import spacy  # For advanced natural language processing tasks
from sklearn.preprocessing import LabelEncoder  # For encoding categorical variables



# Importing modules for model selection and evaluation
from sklearn.model_selection import train_test_split  # For splitting data into training and test sets
from sklearn.model_selection import GridSearchCV, cross_val_predict, StratifiedKFold  # For hyperparameter tuning and cross-validation

# Importing machine learning algorithms
from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier for text classification
from sklearn.linear_model import LogisticRegression  # Logistic Regression model
from sklearn.svm import SVC  # Support Vector Classification

# Importing TensorFlow and Keras for deep learning
import tensorflow as tf  # TensorFlow library for machine learning and deep learning
from tensorflow.keras.models import Sequential  # Sequential model for building neural networks
from tensorflow.keras.layers import Dense, Dropout  # Layers for building a neural network
from tensorflow.keras.optimizers import Adam  # Adam optimizer for training neural networks


# Importing modules for evaluating model performance
from sklearn.metrics import classification_report, accuracy_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score  # Metrics for evaluating classification models
from sklearn.metrics import confusion_matrix, roc_curve, auc  # For evaluating the performance of classification models
from tabulate import tabulate   # For creating tables
import pickle


""" **Data Exploring**"""

#reading dataset
df = pd.read_csv(r'C:\Users\Lenovo\Desktop\Sentiment_Analysis\IMDB Dataset.csv')
df.head()

df.shape

df.info()

df.describe()

df['review'][0]

df['sentiment'].value_counts()

# Plot sentiment distribution
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='sentiment', order=df['sentiment'].value_counts().index)
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()

"""#Implement data cleaning and preprocessing techniques"""

"""
   There are html stripes, noise words and other unnecessary things in reviews
   so I am going to remove them using spacy and regex libraries by creating a function.

"""
# Load a spaCy model
nlp = spacy.load("en_core_web_sm")

def clean_review(review):
    # Remove HTML tags using a simple regex
    review = re.sub(r'<.*?>', '', review)
    # Remove non-alphabetic characters
    review = re.sub(r'[^a-zA-Z\s]', '', review)
    # Convert to lowercase
    review = review.lower()
    # Remove extra whitespaces
    review = re.sub(r'\s+', ' ', review)

    # Process the text with spaCy
    doc = nlp(review)

    cleaned_tokens = []
    for token in doc:
        # Remove stop words, punctuation, URLs, emails, and numbers
        if not token.is_stop and not token.is_punct and not token.like_url and not token.like_email and not token.like_num:

            # Use the lemma of the token
            cleaned_tokens.append(token.lemma_)

    # Rejoin tokens into a cleaned string
    cleaned_review = ' '.join(cleaned_tokens)

    return cleaned_review

# Apply the cleaning function to the 'review' column
df['cleaned_review'] = df['review'].apply(clean_review)

# Display the cleaned reviews
print(df[['review', 'cleaned_review']].head())

df.head()

"""Creating Word Cloud for most frequent words

"""

# Combine all reviews by sentiment
positive_reviews = ' '.join(df[df['sentiment'] == 'positive']['cleaned_review'])
negative_reviews = ' '.join(df[df['sentiment'] == 'negative']['cleaned_review'])

# Generate word clouds
positive_wc = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)
negative_wc = WordCloud(width=800, height=400, background_color='white').generate(negative_reviews)

# Plot positive word cloud
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(positive_wc, interpolation='bilinear')
plt.axis('off')
plt.title('Positive Reviews Word Cloud')

# Plot negative word cloud
plt.subplot(1, 2, 2)
plt.imshow(negative_wc, interpolation='bilinear')
plt.axis('off')
plt.title('Negative Reviews Word Cloud')

plt.show()

"""#Create TF-IDF vectorization for feature extraction"""

# Initialize the TF-IDF Vectorizer
tfidf = TfidfVectorizer(max_features=5000)


# Fit and transform the cleaned reviews
X = tfidf.fit_transform(df['cleaned_review'])

# Save the TF-IDF vectorizer
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)


# Get the target labels
# Convert the 'sentiment' column to numerical values by Labelencoding (0 for negative, 1 for positive)
y = df["sentiment"] = LabelEncoder().fit_transform(df["sentiment"])

# Display the shape of the vectorized data
print(X.shape)
print(y.shape)
#X.head()

df['sentiment'].head()

"""**Splitting Data**"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""#*Train and compare multiple classical ML models (Naive Bayes, Logistic Regression, SVM)*

"""

# Initializing the models
nb = MultinomialNB()
lr = LogisticRegression(max_iter=1000)
svm = SVC()

# Train Naive Bayes
nb.fit(X_train, y_train)
nb_pred = nb.predict(X_test)
print("Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))
print(classification_report(y_test, nb_pred))

#Train Logistic Regression
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
print("Logistic Regression Accuracy:", accuracy_score(y_test, lr_pred))
print(classification_report(y_test, lr_pred))

#Train SVM
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print("SVM Accuracy:", accuracy_score(y_test, svm_pred))
print(classification_report(y_test, svm_pred))


"""#*Saving Best Model*"""

# Save the model
with open('svm_model.pkl', 'wb') as f:
    pickle.dump(svm, f)

# Load the model
with open('svm_model.pkl', 'rb') as f:
    loaded_svm = pickle.load(f)



"""#*Implementing a simple neural network using Keras (compatible with free Colab GPUs)*"""

#Instantiate a Sequential model
model = Sequential()
#Add a Dense layer
model.add(Dense(512, input_shape=(5000,), activation="relu"))
#Add dropout for regularization
model.add(Dropout(0.5))
# Add a hidden layer
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
# Add a second hidden layer
#model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.5))
#output layer
model.add(Dense(1, activation ='sigmoid'))
#Compile model
model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics=['accuracy'])
#fit the data on model
model.fit(X_train,y_train, epochs = 10)

#Evaluate the model on the test set
loss,accuracy = model.evaluate(X_test.toarray(), y_test)
print(f'Neural Network Loss: {loss:.4f}')
print(f'Neural Network Accuracy: {accuracy:.4f}')
y_pred_prob = model.predict(X_test.toarray())  # Predict probabilities
y_pred_neural = (y_pred_prob > 0.5).astype("int32")  # Convert probabilities to binary classes

#creating classification report for evaluation
classification_rep = classification_report(y_test, y_pred_neural, target_names=['Negative', 'Positive'])
print(classification_rep)

"""#*Perform basic hyperparameter tuning and k-fold cross-validation*"""

# Define the parameter grid for each model
param_grid_nb = {
    'alpha': [0.1, 0.5, 1.0]
}

param_grid_lr = {
    'C': [0.01, 0.1, 1, 10],
    #'penalty': ['l2'],
    'solver': ['liblinear', 'lbfgs']
}

param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf']
}

# Initialize the models
nb = MultinomialNB()
lr = LogisticRegression(max_iter=200)
svm = SVC()  # Enable probability estimates for ROC curve

# Initialize GridSearchCV for each model
grid_nb = GridSearchCV(nb, param_grid_nb, cv=2, scoring='accuracy')
grid_lr = GridSearchCV(lr, param_grid_lr, cv=2, scoring='accuracy')
grid_svm = GridSearchCV(svm, param_grid_svm, cv=2, scoring='accuracy')

# Fit the models with GridSearchCV
grid_nb.fit(X_train, y_train)
grid_lr.fit(X_train, y_train)
grid_svm.fit(X_train, y_train)

# Print the best parameters and accuracy for each model
print(f"Best Parameters for Naive Bayes: {grid_nb.best_params_}")
print(f"Best Accuracy for Naive Bayes: {grid_nb.best_score_:.4f}")

print(f"Best Parameters for Logistic Regression: {grid_lr.best_params_}")
print(f"Best Accuracy for Logistic Regression: {grid_lr.best_score_:.4f}")

print(f"Best Parameters for SVM: {grid_svm.best_params_}")
print(f"Best Accuracy for SVM: {grid_svm.best_score_:.4f}")

"""#Create confusion matrices and ROC curves for model evaluation

"""

# y_test and y_pred for each model are defined
y_preds = {
    'Naive Bayes': nb_pred,
    'Logistic Regression': lr_pred,
    'SVM Confusion Matrix': svm_pred,
    'Neural Network': y_pred_neural
}

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Plot confusion matrices for each model
for model_name, y_pred in y_preds.items():
    plot_confusion_matrix(y_test, y_pred, model_name)

"""## ROC curves for model evaluation"""

# Calculate predicted probabilities
nb_probs = nb.predict_proba(X_test)[:, 1]  # Naive Bayes
lr_probs = lr.predict_proba(X_test)[:, 1]  # Logistic Regression
svm_probs = svm.decision_function(X_test)  # SVM (use decision function instead of predict_proba)

# Calculate ROC curves
nb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
svm_fpr, svm_tpr, _ = roc_curve(y_test, svm_probs)

# Calculate AUC scores
nb_auc = auc(nb_fpr, nb_tpr)
lr_auc = auc(lr_fpr, lr_tpr)
svm_auc = auc(svm_fpr, svm_tpr)

# Plot ROC curves
plt.figure(figsize=(10, 8))
plt.plot(nb_fpr, nb_tpr, color='blue', label=f'Naive Bayes (AUC = {nb_auc:.2f})')
plt.plot(lr_fpr, lr_tpr, color='green', label=f'Logistic Regression (AUC = {lr_auc:.2f})')
plt.plot(svm_fpr, svm_tpr, color='red', label=f'SVM (AUC = {svm_auc:.2f})')

# Plotting random chance line
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')


print(f'Naive Bayes AUC {nb_auc:.2f}')
print(f'Logistic Regression AUC: {lr_auc:.2f}')
print(f'SVM AUC: {svm_auc:.2f}')

# Customizing the plot
plt.title('ROC Curves')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')

# Show plot
plt.show()

# Compute ROC curve and AUC for Neural Network
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

classification_rep, conf_matrix, roc_auc

# Plotting the ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

"""#Results"""

# Create the data for the table, rounding the values to 4 decimal places
data = [
    ["Naive Bayes", 0.8469, 0.8449, 0.8527, 0.8488, 0.9233],
    ["Logistic Regression", 0.8798, 0.8686, 0.8972, 0.8827, 0.9524],
    ["SVM", 0.8845, 0.8719, 0.9036, 0.8874, 0.9555],
    ["Neural Network", 0.8716,  0.8566, 0.8950, 0.8754, 0.9427]
]

# Define the column headers
headers = ["Model", "Accuracy", "Precision", "Recall", "F1-Score", "AUC"]

# Print the table using tabulate with rounding
print(tabulate(data, headers, tablefmt="pretty", floatfmt=".4f"))

"""# Comparing Results


*  **Best Overall Performance:** Support Vector Machine (SVM) shows the highest scores in precision, recall, F1-Score, and AUC, making it the most robust model in terms of both classification and differentiation performance.
*  However, all models demonstrated strong classification abilities, with each having specific strengths.
* **Logistic Regression** and the **Neural Network** were also top contenders, offering a good balance across all metrics.
* **Naive Bayes,** while slightly behind in some areas, still provided robust performance with an AUC close to 0.92.


"""

